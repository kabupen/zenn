---
title: ""
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---



# VAE 

Variational Autoencoder（VAE）と呼ばれるモデルが提案されたのは 2014 年の [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) です。
ただ VAE の何がすごいのか、Stable Diffusion などの前処理でもいまだ VAE が使用されていますが、何が凄いのか、よく分からなかったので本稿でその背景に迫ってみたいと思います。


Autoencoder との比較記事などがあり、「え、VAEは AutoencoderのSOTAなの？」と思ったりしていたのですが、特に元論文にそのような記載もないので混乱していました。



なぜ潜在変数を考えているのか？



Autoencoder に非線形変換を加えたモデルが VAE です。そのため複雑な構造をもつデータのモデル化が可能になります。


# 流れ

- 最尤推定とベイズ推定
  - 点推定か確率推定か？ $\theta$ か、$p(\theta)$ か？
  - 最尤推定とは likelihood を "最大化" することでパラメータを求める（likelihoodを使うことがイコール最尤法ではない）


- 変分推論
  - ELBO
- 平均場近似
- EMアルゴリズム
- 変分EMアルゴリズム
  - 何が変分なのか？

- 最尤推定的な取り扱い
- MAP


# Variational Autoencoder


$\beta$ VAE、FactorVAE、$\beta$-TCVAE
DIPVAE
JointVAE
CascadeVAE

#


最尤法ではパラメータ $\theta$ は決定論的に値が決まるものとして扱います
ベイズ的取り扱いでは、パラメータ $\theta$ は確率変数として扱います

- fully Beysian model では全ての未知変数には事前分布が与えられる


ベイズ推論を用いた手法では確率モデルを設計することになります。

$$
p(X, Z)
$$

学習や予測などの具体的な課題は事後分布 $p(Z|X)$ を計算することで実現できます。


## 線形モデル

線形モデルを例に、最尤推定的な取り扱いと、ベイズ的な取り扱いについて考察しておきたいと思います^[「ベイズ的な取り扱い」のしっくりくる単語が無いような気がしますが...。]。

### 最尤推定

パラメータ $\theta$ が求まる

### ベイズ推定


事後分布 $p(\theta|X)$ が求まる



#  混合ガウス分布

潜在変数を持つモデルについて、混合ガウス分布を題材にしてみます。


確率分布を線形結合して作る確率分布を混合分布（mixture distribution）と呼びます。このときにガウス分布を使用したモデルを混合ガウス分布と呼び、$K$ 個のガウス分布の重ね合わせ

$$
p(\rm{x}) = \sum_{k=1}^K \pi_k N(\rm{x}|\mu_k, \Sigma_k)
$$

で表されます。それぞれ個別に平均と共分散のパラメータを持っています。

十分な数のガウス分布を用いて、係数と平均・共分散を調節すれば、ほぼ任意の連続な密度関数を任意の精度で近似できます。


## 問題

混合ガウス分布が有用であるということは分かったのですが、ではそれらのパラメータをどのように求めれば良いのでしょうか？ここでEMアルゴリズムと呼ばれる枠組みを用いることができます。


対数尤度関数は

$$
\ln p(\rm{X}|\pi, \mu, \Sigma) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N(\rm{x}_n|\mu_k, \Sigma_k) \right\}
$$

となるのですが、尤度関数はもはや解析的に計算できなくなってしまっています。そこで混合ガウス分布に潜在変数を導入して、EMアルゴリズムの枠組みを使用することでパラメータ推定が可能となります。




#  EM アルゴリズム

EMアルゴリズムを題材として、最尤推定とベイズ推定の違いについて更に深ぼって見ていきたいと思います。根本の違いはそれぞれの推定方法の差異であるモデルパラメータの取り扱いに起因します。



## 最尤法的な取り扱い （通常のEMアルゴリズム）


EM アルゴリズムは最尤推定の計算方法の一つで、潜在変数を持つモデルの likelihood を最大化するパラメータ $\theta$ を求めることを目的としています。

観測した変数と潜在変数のどちらの情報も分かっていれば（$X, Z$ は complete dataset と呼ばれます）^[こうなると最早 $Z$ が "潜在" 変数ではないのですが...。]、

$$
\argmax_\theta \ln p(X, Z|\theta)
$$

の joint distribution $p(X, Z|\theta)$ の対数尤度からパラメータ $\theta$ を決定するのが straightforward な手法です。ただし実際には $Z$ がどのような値を取って $X$ と関係しているか分からないです（$X$ だけだと incomlete data と呼ばれます）。

$$
\ln p(X, Z|\theta) = \ln \left( \Sum_Z p(X, Z|\theta) \right) = \ln p(X|\theta)
$$

ここで $p(X|\theta)$ の最適化は難しいが、$p(X,Z|\theta)$ の最適化は比較的簡単である場合を想定します。


ベイズ推定 = 事前分布を導入すること、と思っているとここで「あれ、EMアルゴリズムは最尤法であると言っておきながら、事前分布 $q(Z)$ が出てきた？？」となってしまいます。が、パラメータ $\theta$ の決定的な値を求めるのだ、ということを頭に再度入れておきましょう。


## ベイズ的な取り扱い （変分EMアルゴリズム）


### 変分法

ベイズ推定とは関係ない（ベイズ推定のための手法ではない）のですが、ここで一旦変分法についてまとめておきます。

### 変分EMアルゴリズム

fully Beysian ですので、汎関数の最適化を解く必要があり、そのため変分法を使用していきます^[汎関数となったのは、パラメータ$\theta$にも事前分布が与えられて、すべて $Z$ に吸収されたためです。]。










# メモ

-
- モデルエビデンス
  - モデルエビデンスの推論方法の一つ
- Reparametrization trick
  - サンプリングで誤差逆伝搬できない問題に対処する方法
- 点推定の欠点（=最尤法の欠点）
  - 単峰ではない分布などを表現できず、過学習になる --> ベイズ推定は確率的に予測し、誤差を表示できる