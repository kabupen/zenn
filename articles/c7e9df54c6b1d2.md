---
title: "MLP + CNN + ViT ナナメ読み"
emoji: "🐕"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["機械学習"]
published: false
---



FCNN vs CNN vs ViT


画像認識のモデルと言えば Convolutional Neural Network（CNN）かと思います。もし、「もっと新しいアーキテクチャで実装してよ」と言われれば、Vision Transformer（ViT）などの系統を候補に入れることと思います。

ただ立ち返って考えると、なぜ CNN や ViT の精度がいいのか。なぜ単純な Multi Layer Perceptron（MCP）ではだめなのか。
この辺りを正しく議論できる人がどれくらいいるのでしょうか？



# モデル各論

## MLP


### ユニバーサル近似定理 (Universal Appliximation Theorems; UAT)

幅（width）を十分に取った 1隠れ層 MLPでは、活性化が非多項式であれば任意の連続関数を近似できることが知られています。



任意精度で近似できることの理論的背景は [A Survey on Universal Approximation Theorems](https://arxiv.org/abs/2407.12895) にまとまっており、テイラー展開、フーリエ級数、ウィーアシュトラス、Kolmogorov–Arnold 表現など古典近似理論を起点に、1989 年の Cybenko／Hornik‐White 以降の UAT、最近の幅最小定理まで議論されています。



| 段階             | 主なアイデア                                                   | 代表定理・論文                                                                |
| -------------- | -------------------------------------------------------- | ---------------------------------------------------------------------- |
| **古典解析**       | 多項式・三角関数による近似                                            | Weierstrass (1885), Fourier (1807)                                     |
| **高次元への拡張**    | 多変量連続関数を有限個の一変量関数へ分解                                     | Kolmogorov–Arnold (1957)                                               |
| **ニューラルネット導入** | 活性化が非多項式なら 1 隠れ層で関数空間に稠密                                 | Cybenko (1989)､ Hornik-White (1989) ([シュプリンガーリンク][1], [サイエンスダイレクト][2]) |
| **密度証明の鍵**     | (i) 非線形活性化であること（非多項式性）<br>(ii) 重み付き和で積分表現(リッツ＝マルコフ表現)を再現 | Leshno et al. 1993（非多項式⇔普遍性） ([ウィキペディア][3])                            |

[1]: https://link.springer.com/article/10.1007/BF02551274?utm_source=chatgpt.com "Approximation by superpositions of a sigmoidal function"
[2]: https://www.sciencedirect.com/science/article/pii/0893608089900208?utm_source=chatgpt.com "Multilayer feedforward networks are universal approximators"
[3]: https://en.wikipedia.org/wiki/Universal_approximation_theorem?utm_source=chatgpt.com "Universal approximation theorem"



#### width, depth

MLP では隠れ層の数（深さ、depth）と、各隠れ層の次元（幅、width）でネットワークの構造が定義されます。
では UAT の文脈においてどういったネットワーク構造を定義すればいいのかというのが、width, depth の場合において議論されてきました。

![](https://storage.googleapis.com/zenn-user-upload/c1f74e3cf92e-20250525.png =500x)



| 視点              | 代表結果                                                                            | 含意                                 |
| --------------- | ------------------------------------------------------------------------------- | ---------------------------------- |
| **浅層（深さ＝2）で十分** | Cybenko / Hornik (1989)：1 隠れ層＋十分な幅 → 普遍近似                                       | “層は 1 でよいが幅は多く必要”                  |
| **幅を制限した深層**    | Lu et al. 2017：入力次元 *n* のとき **幅 n + 4** の ReLU ネットで任意 $L^1$ 関数を近似可 ([arXiv][1]) | 幅を常識的に抑えつつ深さで表現力を担保                |
| **最小幅の決定**      | Park et al. 2021：ReLU の最小幅は **max{ n + 1, 出力次元 }** と証明 ([arXiv][2])             | 幅 n のネットではほぼ全ての関数を近似できない（測度零集合を除く） |
| **幅＝1 限界**      | Lu et al. 2017：幅 ≤ n では（ほとんど）普遍近似不可能 ([arXiv][1])                               | “ディープだが超細い”ネットは原理的に表現力不足           |

[1]: https://arxiv.org/abs/1709.02540?utm_source=chatgpt.com "The Expressive Power of Neural Networks: A View from the Width"
[2]: https://arxiv.org/abs/2006.08859?utm_source=chatgpt.com "Minimum Width for Universal Approximation"


要するに

- 理論的には隠れ層は1層で十分だが、その代わり width（隠れ層の次元） が指数的に増えうる
- 実用上は width を抑え、depth（層数） を増やすほうがパラメーター効率がよい







<!-- ============================================== -->






## CNN


Convolutional Neural Network（CNN）は 2012 年の AlexNet の登場以降、その汎化性能の高さから画像認識モデルのデファクトスタンダードとなっています。
CNN を特徴づける性質として

- 平行移動不変性 (translation invariance)
- 表現学習能力
- フィルタ共有によるパラメータ効率
- 局所特徴量のみ取得

が存在します。

### 平行移動不変性


ある演算 $F$ が translation equivariant とは、入力信号 $x$ が平行移動（シフト）したあとに $F$ を適用した結果が

$$
F(T_Delta x) = T_\Delta x = T_\Delta x = T_\Delta x = T_\Delta x
$$

一致することとをいいます。

ニューラルネットワークを用いた物体認識でもこの不変性を獲得するために、古くから畳み込みやプーリング層を用いた技術発展がなされてきました。

物体を見る際に人間であれば、その物体が網膜上にどう映っても認識することが可能であるので、その意味では人間は平行移動不変性を持った物体認識を行えていることになります。

CNN が並行移動不変性について頑強であるとのことは以下で議論されてきており ^[Han et al. "Scale and translation-invariance for novel objects in human vision"]、

> Convolutional Neural Networks, which are widely used in computer vision, have an architecture hard-wired for some translation-invariance while they rely heavily on learning through extensive data or data augmentation for invariance to other transformations.
> _コンピュータビジョンで広く用いられる畳み込みニューラルネットワーク（CNN）は、アーキテクチャ自体にある程度の平行移動に対する不変性が組み込まれている一方で、それ以外の変換（回転や拡大縮小など）に対する不変性は、大量のデータ学習やデータ拡張に大きく依存している。_
 
注意点としてはここで議論されているように、あくまで平行移動に関してそのネットワークの構造上頑健であるが、それ以外の変換には対応できていないことです。


#### さらなる議論

一般的に CNN は平行移動不変性を持っているとされているのですが、そもそもどの機構によって平行移動不変性が獲得されているかといった研究は少ないのです。

https://arxiv.org/pdf/2110.05861 で引用されている先行研究である [Quantifying Translation-Invariance in Convolutional Neural Networks](https://arxiv.org/abs/1801.01450) によると、

> CNNs are not inherently translation-invariant by virtue of their architecture alone, however they have the capacity to learn translation-invariant representations if trained on appropriate data.
> _しかし、平行移動不変性がどの機構によって得られるのかについての決定的な説明は存在しない。_

とあり、学習データをデータ拡張（augmentation）を施して平行移動によるバリエーションをもたせることが最重要要因であったとされています。深いネットワークや大きなフィルタは、拡張データで訓練したときに限りより強い不変性を学習しうるとのことです。

そのため一般的に言われているような

- 層が深くなるにつれて受容野が拡大するため
- サブサンプリング（プーリング）が不変性をもたらす

CNN が構造的にそもそも平行移動不変性を持ち合わせているという議論に関しては、まだ研究段階であると感じます。


---

- \[1\] [Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be](https://arxiv.org/pdf/2110.05861)
- \[2\] [Quantifying Translation-Invariance in Convolutional Neural Networks](https://arxiv.org/abs/1801.01450)








translation equivalence, translation invariant とも表現される性質で、

CNN では、畳み込み（convolution）はカーネル重みを全空間で共有しており、この式を満たします。そのため、入力画像がずれても、その分だけ出力特徴量マップもずれることとなり、学習したフィルタが画像内のどこに現れても同じように応答するため、位置不変なパターン検出器として働きます。



### archices
CNN
空間的な局所構造への帰納バイアス（平行移動不変性、局所相関など）を持つため、画像という構造化データでは少ないパラメータで効果的に学習できます。
一方で MLP は全結合層のみで構成され、画像中のピクセル間の局所的な関連を事前に仮定しないので同程度の性能を引き出すにはより大規模なモデルとデータが必要となります。そのため、一般的な規模のデータセットではCNNがMLPより高精度になりやすいとされています。














### 大域的情報について


局所受容野

- https://arxiv.org/pdf/1906.05909



![](https://storage.googleapis.com/zenn-user-upload/5599a09f8810-20250527.png)




CNN では局所特徴量に偏り、入力情報の大域的な情報を捉えそこねることが指摘されています。




### Texture bias


- The Origins and Prevalence of Texture Bias in Convolutional Neural Networks



ImageNet で学習された CNN では物体を認識する際に形状（shape）よりも質感（texture）を手がかりに分類判断を行う傾向があることが分かっています。形状と質感が意図的に食い違う画像セット（例：象の皮膚テクスチャをまとった猫）を用いた結果、CNN は質感にもとづき（例：「ゾウ」）分類する傾向が観察されています ^[ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness]。

![](https://storage.googleapis.com/zenn-user-upload/a67b8a435928-20250528.png)


#### 原因

（１）データセット起因

> A hypothesis consistent with the results above is that texture bias is driven by the joint image-label statistics of the ImageNet dataset. To correctly label the many dog breeds in the dataset, for instance, a model would have to make texture-based distinctions between similarly shaped objects. 
> _§6 Effect of training objective, "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks"_

似たような形状のクラスを学習するためには被写体の形状ではなくより細かいパターン（テクスチャ）を学習するほうが効率的になってしまいます。ここからネットワークが texture bias を持ってしまうという議論があります。

（２）Augmentation起因

> We hypothesized that random-crop augmentation might remove global shape information from the image, since for large central objects, randomly varying parts of the object’s shape may appear in the crop, rendering shape a less reliable feature relative to texture. 
> _§5 The role of data augmentation in texture bias, "[The Origins and Prevalence of Texture Bias in Convolutional Neural Networks](https://arxiv.org/pdf/1911.09071)"_

ランダムクロップと、照明系の augmentation についての議論から、augmentaiton によって様々なバイアスが出てしまうことが分かっています。
標準のデータ拡張では画像をランダムに切り出すのですが、このとき大きな物体は一部しか残らず、全体輪郭（形状）が見えづらくなる一方、局所的な質感パターン（毛・木目など）は依然として残ります。結果としてモデルは形よりテクスチャを頼りに分類しやすくなるようです。

> The majority of photographs in major computer vision datasets are taken in well-lit environments. Humans encounter much more varied illumination conditions. We hypothesized that the development of human-like shape representations might require more diverse training data than what is present in ImageNet
> _§5 The role of data augmentation in texture bias, "[The Origins and Prevalence of Texture Bias in Convolutional Neural Networks](https://arxiv.org/pdf/1911.09071)"_

ImageNet 画像は比較的均一な条件で撮影されているので、照明や色が似通っていると形状よりも安定して観測できるテクスチャ特徴が有利になり、これもバイアスを後押しするようになります。


（３）最終層付近での学習起因


> Interestingly, shape accuracy decreased through the fully-connected layers of AlexNet’s classifier, and from ResNet pre-pool to post-pool, suggesting that these models’ classification layers remove shape information.
> _§8 Degree of representation of shape and texture in ImageNet models, "[The Origins and Prevalence of Texture Bias in Convolutional Neural Networks](https://arxiv.org/pdf/1911.09071)"_

どうやら最終層の全結合層付近で形状情報が捨てられる傾向にあるとのことです（これがなぜなのか、説明はなされていませんが...）。

<!-- ============================================== -->
## MLP-Mixer



CNN は局所受容野を用いて画像から特徴を抽出するので、画像全体にまたがるような長距離の依存関係や大局的な文脈の捉え方には限界がある。









<!-- ============================================== -->












# 実用上において


- MLP
- CNN
- ViT


実用上はCNNの構造的利点によって、少ないデータで高精度を達成できるため、現在まで多くのタスクではCNNが優位に立ってきていました。



<!-- 参考文献メモ -->
